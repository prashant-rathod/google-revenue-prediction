{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /usr/local/lib/python3.6/site-packages (3.2.1)\n",
      "Requirement already satisfied: pytz in /Users/sandy/Library/Python/3.6/lib/python/site-packages (from plotly) (2018.4)\n",
      "Requirement already satisfied: requests in /Users/sandy/Library/Python/3.6/lib/python/site-packages (from plotly) (2.18.4)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/site-packages (from plotly) (1.3.3)\n",
      "Requirement already satisfied: six in /Users/sandy/Library/Python/3.6/lib/python/site-packages (from plotly) (1.11.0)\n",
      "Requirement already satisfied: decorator>=4.0.6 in /usr/local/lib/python3.6/site-packages (from plotly) (4.3.0)\n",
      "Requirement already satisfied: nbformat>=4.2 in /usr/local/lib/python3.6/site-packages (from plotly) (4.4.0)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/sandy/Library/Python/3.6/lib/python/site-packages (from requests->plotly) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/sandy/Library/Python/3.6/lib/python/site-packages (from requests->plotly) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/sandy/Library/Python/3.6/lib/python/site-packages (from requests->plotly) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sandy/Library/Python/3.6/lib/python/site-packages (from requests->plotly) (2018.4.16)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (2.6.0)\n",
      "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (4.3.2)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (4.4.0)\n",
      "Requirement already up-to-date: pip in /usr/local/lib/python3.6/site-packages (18.1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from sklearn import preprocessing\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read the data from our clean CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180430\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/Users/sandy/Documents/ENPM808/Project/Github/google-revenue-prediction/train.csv\")\n",
    "test_df = pd.read_csv(\"/Users/sandy/Documents/ENPM808/Project/Github/google-revenue-prediction/test.csv\")\n",
    "\n",
    "print(max(train_df['date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split date into day month year and weekday! (Preprocessing does this now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='ignore')\n",
    "    df['year'] = df['date'].apply(lambda x: x.year)\n",
    "    df['month'] = df['date'].apply(lambda x: x.month)\n",
    "    df['day'] = df['date'].apply(lambda x: x.day)\n",
    "    df['weekday'] = df['date'].apply(lambda x: x.weekday())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_time_features(train_df)\n",
    "test_df = add_time_features(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's modify the data a bit. We need to encode the string type columns into numerical type using label encoding. We will also split the train and dev data and remove some columns that seem to be unimportant.\n",
    "\n",
    "Since the target value has such a huge range, lets take the log and predict for the log of transactionRevenue. It will be reversed using exponent function before calculating RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channelGrouping\n",
      "device.browser\n",
      "device.deviceCategory\n",
      "device.operatingSystem\n",
      "geoNetwork.city\n",
      "geoNetwork.continent\n",
      "geoNetwork.country\n",
      "geoNetwork.metro\n",
      "geoNetwork.networkDomain\n",
      "geoNetwork.region\n",
      "geoNetwork.subContinent\n",
      "trafficSource.adContent\n",
      "trafficSource.adwordsClickInfo.adNetworkType\n",
      "trafficSource.adwordsClickInfo.gclId\n",
      "trafficSource.adwordsClickInfo.page\n",
      "trafficSource.adwordsClickInfo.slot\n",
      "trafficSource.campaign\n",
      "trafficSource.keyword\n",
      "trafficSource.medium\n",
      "trafficSource.referralPath\n",
      "trafficSource.source\n",
      "trafficSource.adwordsClickInfo.isVideoAd\n",
      "trafficSource.isTrueDirect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:43: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the\n",
      "'datetime.date' is coerced to a datetime. In the future pandas will\n",
      "not coerce, and a TypeError will be raised. To retain the current\n",
      "behavior, convert the 'datetime.date' to a datetime with\n",
      "'pd.Timestamp'.\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:44: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the\n",
      "'datetime.date' is coerced to a datetime. In the future pandas will\n",
      "not coerce, and a TypeError will be raised. To retain the current\n",
      "behavior, convert the 'datetime.date' to a datetime with\n",
      "'pd.Timestamp'.\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:45: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the\n",
      "'datetime.date' is coerced to a datetime. In the future pandas will\n",
      "not coerce, and a TypeError will be raised. To retain the current\n",
      "behavior, convert the 'datetime.date' to a datetime with\n",
      "'pd.Timestamp'.\n"
     ]
    }
   ],
   "source": [
    "# Impute 0 for missing target values\n",
    "train_df[\"totals.transactionRevenue\"].fillna(0, inplace=True)\n",
    "train_y = train_df[\"totals.transactionRevenue\"].values\n",
    "train_id = train_df[\"fullVisitorId\"].values\n",
    "test_df[\"totals.transactionRevenue\"].fillna(0, inplace=True)\n",
    "test_y = test_df[\"totals.transactionRevenue\"].values\n",
    "test_id = test_df[\"fullVisitorId\"].values\n",
    "test_id = test_df[\"fullVisitorId\"].values\n",
    "\n",
    "\n",
    "# label encode the categorical variables and convert the numerical variables to float \n",
    "# scikit.rf needs numerical data. One hot encoding is not good on rf.\n",
    "cat_cols = [\"channelGrouping\", \"device.browser\", \n",
    "            \"device.deviceCategory\", \"device.operatingSystem\", \n",
    "            \"geoNetwork.city\", \"geoNetwork.continent\", \n",
    "            \"geoNetwork.country\", \"geoNetwork.metro\",\n",
    "            \"geoNetwork.networkDomain\", \"geoNetwork.region\", \n",
    "            \"geoNetwork.subContinent\", \"trafficSource.adContent\", \n",
    "            \"trafficSource.adwordsClickInfo.adNetworkType\", \n",
    "            \"trafficSource.adwordsClickInfo.gclId\", \n",
    "            \"trafficSource.adwordsClickInfo.page\", \n",
    "            \"trafficSource.adwordsClickInfo.slot\", \"trafficSource.campaign\",\n",
    "            \"trafficSource.keyword\", \"trafficSource.medium\", \n",
    "            \"trafficSource.referralPath\", \"trafficSource.source\",\n",
    "            'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.isTrueDirect']\n",
    "\n",
    "for col in cat_cols:\n",
    "    print(col)\n",
    "    lbl = preprocessing.LabelEncoder()    \n",
    "    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n",
    "    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n",
    "    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))\n",
    "\n",
    "#convert integer columns to float.\n",
    "num_cols = [\"totals.pageviews\", \"visitNumber\", \"visitStartTime\", 'totals.bounces',  'totals.newVisits']    \n",
    "for col in num_cols:\n",
    "    train_df[col] = train_df[col].astype(float)\n",
    "    test_df[col] = test_df[col].astype(float)\n",
    "\n",
    "# Split the train dataset into development and valid based on time \n",
    "dev_df = train_df[train_df['date']<=datetime.date(2018, 1, 1)]\n",
    "dev_df = dev_df[dev_df['date']>=datetime.date(2017, 1, 1)]\n",
    "val_df = train_df[train_df['date']>datetime.date(2018, 1, 1)]\n",
    "dev_y = np.log1p(dev_df[\"totals.transactionRevenue\"].values)\n",
    "val_y = np.log1p(val_df[\"totals.transactionRevenue\"].values)\n",
    "\n",
    "#exclude irrelevant data like ID and also target variables from train data!\n",
    "cols_to_exclude = ['totals.transactionRevenue','totals.totalTransactionRevenue','totals.transactions','date','fullVisitorId', 'visitId']\n",
    "dev_X = dev_df.copy()\n",
    "val_X = val_df.copy()\n",
    "test_X = test_df.copy()\n",
    "dev_X.drop(cols_to_exclude, axis=1, inplace=True)\n",
    "val_X.drop(cols_to_exclude, axis=1, inplace=True)\n",
    "test_X.drop(cols_to_exclude, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a baseline!\n",
    "\n",
    "Taking all zero's is not exactly right since although most values are zeroes, we are finally looking at revenue per customer (not per visit). So let's take the mean revenue of all customers.\n",
    "\n",
    "RMSE: 13.85\n",
    "\n",
    "That's pretty bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.850822744551245\n",
      "CPU times: user 560 ms, sys: 70.5 ms, total: 631 ms\n",
      "Wall time: 637 ms\n"
     ]
    }
   ],
   "source": [
    "#BASELINE!\n",
    "\n",
    "%%time\n",
    "val_pred_rf1 = pd.DataFrame({\"fullVisitorId\":val_df[\"fullVisitorId\"].values})\n",
    "val_pred_rf1[\"transactionRevenue\"] = val_df[\"totals.transactionRevenue\"].values\n",
    "\n",
    "val_pred_rf1 = val_pred_rf1.groupby(\"fullVisitorId\")[\"transactionRevenue\"].sum().reset_index()\n",
    "pred_val_group_mean = val_pred_rf1[\"transactionRevenue\"].mean()\n",
    "val_pred_rf1[\"PredictedRevenue\"] = pred_val_group_mean\n",
    "print(np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_rf1[\"transactionRevenue\"].values), np.log1p(val_pred_rf1[\"PredictedRevenue\"].values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a random forest to see if it helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              importance\n",
      "totals.pageviews                                0.219648\n",
      "totals.timeOnSite                               0.118154\n",
      "visitStartTime                                  0.112671\n",
      "totals.hits                                     0.065865\n",
      "day                                             0.065470\n",
      "geoNetwork.country                              0.063355\n",
      "totals.sessionQualityDim                        0.052230\n",
      "weekday                                         0.036666\n",
      "visitNumber                                     0.036373\n",
      "geoNetwork.networkDomain                        0.035648\n",
      "geoNetwork.city                                 0.028542\n",
      "trafficSource.referralPath                      0.024326\n",
      "device.operatingSystem                          0.020930\n",
      "geoNetwork.region                               0.018018\n",
      "geoNetwork.metro                                0.016730\n",
      "month                                           0.013177\n",
      "trafficSource.isTrueDirect                      0.011275\n",
      "device.browser                                  0.008755\n",
      "totals.newVisits                                0.008309\n",
      "channelGrouping                                 0.008019\n",
      "device.deviceCategory                           0.005227\n",
      "trafficSource.source                            0.004958\n",
      "trafficSource.keyword                           0.004927\n",
      "device.isMobile                                 0.004730\n",
      "trafficSource.adwordsClickInfo.gclId            0.004512\n",
      "trafficSource.medium                            0.004476\n",
      "geoNetwork.subContinent                         0.001510\n",
      "trafficSource.adContent                         0.001481\n",
      "trafficSource.campaign                          0.001349\n",
      "geoNetwork.continent                            0.000882\n",
      "trafficSource.adwordsClickInfo.slot             0.000483\n",
      "trafficSource.adwordsClickInfo.adNetworkType    0.000442\n",
      "trafficSource.adwordsClickInfo.isVideoAd        0.000417\n",
      "trafficSource.adwordsClickInfo.page             0.000408\n",
      "year                                            0.000027\n",
      "totals.bounces                                  0.000011\n"
     ]
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 100 decision trees\n",
    "\n",
    "def run_rf(train_X, train_y, val_X, val_y, test_X):\n",
    "    rf = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "    rf.fit(train_X, train_y);\n",
    "    feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = train_X.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "    print (feature_importances)\n",
    "    pred_val_y = rf.predict(val_X)\n",
    "    pred_test_y = rf.predict(test_X)\n",
    "    return rf, pred_val_y, pred_test_y\n",
    "# Train the model on training data\n",
    "rf, pred_val_rf, pred_test_rf = run_rf(dev_X, dev_y, val_X, val_y, test_X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5171902108244966\n"
     ]
    }
   ],
   "source": [
    "#validation\n",
    "from sklearn import metrics\n",
    "pred_val_rf[pred_val_rf<0] = 0\n",
    "val_pred_rf = pd.DataFrame({\"fullVisitorId\":val_df[\"fullVisitorId\"].values})\n",
    "val_pred_rf[\"transactionRevenue\"] = val_df[\"totals.transactionRevenue\"].values\n",
    "val_pred_rf[\"PredictedRevenue\"] = np.expm1(pred_val_rf)\n",
    "val_pred_rf = val_pred_rf.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\n",
    "print(np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_rf[\"transactionRevenue\"].values), np.log1p(val_pred_rf[\"PredictedRevenue\"].values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1631782950753666\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "pred_test_rf[pred_test_rf<0] = 0\n",
    "test_pred_rf = pd.DataFrame({\"fullVisitorId\":test_df[\"fullVisitorId\"].values})\n",
    "test_pred_rf[\"transactionRevenue\"] = test_df[\"totals.transactionRevenue\"].values\n",
    "test_pred_rf[\"PredictedRevenue\"] = np.expm1(pred_test_rf)\n",
    "test_pred_rf = test_pred_rf.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\n",
    "print(np.sqrt(metrics.mean_squared_error(np.log1p(test_pred_rf[\"transactionRevenue\"].values), np.log1p(test_pred_rf[\"PredictedRevenue\"].values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty impressive! A really nominally tuned random forest did loads better than baseline!\n",
    "\n",
    "Since this is promising, let's try to tune those hyper parameters some more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [20, 51, 82, 113, 144, 175, 206, 237, 268, 300], 'max_features': ['auto', 'sqrt'], 'max_depth': [6, 10, 15, 20, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 300, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(6, 20, num = 4)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 68.2min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 284.6min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 607.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [20, 51, 82, 113, 144, 175, 206, 237, 268, 300], 'max_features': ['auto', 'sqrt'], 'max_depth': [6, 10, 15, 20, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(dev_X, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 300,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': None,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took 12 hours! And we've got ourselves a new set of hyperparameter values.\n",
    "\n",
    "Phew! Now let's see how much of a difference all that computing made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                importance\n",
      "totals.pageviews                              1.892682e-01\n",
      "totals.hits                                   1.628767e-01\n",
      "totals.timeOnSite                             1.073618e-01\n",
      "totals.sessionQualityDim                      9.738054e-02\n",
      "visitStartTime                                6.867285e-02\n",
      "day                                           4.158431e-02\n",
      "visitNumber                                   3.140015e-02\n",
      "geoNetwork.country                            3.008232e-02\n",
      "month                                         2.741987e-02\n",
      "geoNetwork.networkDomain                      2.470687e-02\n",
      "weekday                                       2.427301e-02\n",
      "trafficSource.referralPath                    2.285246e-02\n",
      "geoNetwork.city                               2.257888e-02\n",
      "geoNetwork.metro                              1.738148e-02\n",
      "geoNetwork.continent                          1.653013e-02\n",
      "geoNetwork.region                             1.585994e-02\n",
      "device.operatingSystem                        1.517795e-02\n",
      "totals.newVisits                              1.159870e-02\n",
      "channelGrouping                               1.153176e-02\n",
      "geoNetwork.subContinent                       9.734556e-03\n",
      "trafficSource.medium                          8.103232e-03\n",
      "trafficSource.isTrueDirect                    7.424426e-03\n",
      "trafficSource.source                          6.975799e-03\n",
      "device.browser                                5.908901e-03\n",
      "device.isMobile                               5.849205e-03\n",
      "device.deviceCategory                         5.645453e-03\n",
      "trafficSource.keyword                         3.735635e-03\n",
      "totals.bounces                                3.207433e-03\n",
      "trafficSource.adwordsClickInfo.gclId          2.094256e-03\n",
      "trafficSource.campaign                        6.621784e-04\n",
      "trafficSource.adContent                       4.820434e-04\n",
      "trafficSource.adwordsClickInfo.adNetworkType  4.167118e-04\n",
      "trafficSource.adwordsClickInfo.slot           4.131053e-04\n",
      "trafficSource.adwordsClickInfo.isVideoAd      4.085707e-04\n",
      "trafficSource.adwordsClickInfo.page           3.997442e-04\n",
      "year                                          8.901254e-07\n",
      "CPU times: user 5min 15s, sys: 3.9 s, total: 5min 19s\n",
      "Wall time: 5min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_random = rf_random.best_estimator_\n",
    "\n",
    "def run_rf_model(model, train_X, train_y, val_X, val_y, test_X):\n",
    "    #rf = RandomForestRegressor(n_estimators = 100, max_depth = 6, min_samples_split = , max_features = \"sqrt\", random_state = 42)\n",
    "    model.fit(train_X, train_y);\n",
    "    \n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                   index = train_X.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "    print (feature_importances)\n",
    "    return rf, pred_val_y, pred_test_y\n",
    "# Train the model on training data\n",
    "rf, pred_val_rf, pred_test_rf = run_rf_model(best_random, dev_X, dev_y, val_X, val_y, test_X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4320818834553026\n",
      "CPU times: user 546 ms, sys: 133 ms, total: 679 ms\n",
      "Wall time: 912 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#validation\n",
    "\n",
    "pred_val_rf[pred_val_rf<0] = 0\n",
    "val_pred_rf = pd.DataFrame({\"fullVisitorId\":val_df[\"fullVisitorId\"].values})\n",
    "val_pred_rf[\"transactionRevenue\"] = val_df[\"totals.transactionRevenue\"].values\n",
    "val_pred_rf[\"PredictedRevenue\"] = np.expm1(pred_val_rf)\n",
    "val_pred_rf = val_pred_rf.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\n",
    "print(np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_rf[\"transactionRevenue\"].values), np.log1p(val_pred_rf[\"PredictedRevenue\"].values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.017829627946382\n",
      "CPU times: user 655 ms, sys: 87.1 ms, total: 742 ms\n",
      "Wall time: 776 ms\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "pred_test_rf[pred_test_rf<0] = 0\n",
    "test_pred_rf = pd.DataFrame({\"fullVisitorId\":test_df[\"fullVisitorId\"].values})\n",
    "test_pred_rf[\"transactionRevenue\"] = test_df[\"totals.transactionRevenue\"].values\n",
    "test_pred_rf[\"PredictedRevenue\"] = np.expm1(pred_test_rf)\n",
    "test_pred_rf = test_pred_rf.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\n",
    "print(np.sqrt(metrics.mean_squared_error(np.log1p(test_pred_rf[\"transactionRevenue\"].values), np.log1p(test_pred_rf[\"PredictedRevenue\"].values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the best so far! And since we're working in the log space, I would say that's pretty impressive.\n",
    "\n",
    "Training time of 6 minutes isn't that terrible either!\n",
    "\n",
    "Let's see if there are any other models that do as well and then decide if this is worth exploring further. I don't want to tune my hyperparameters for another 12 hours if there is a better model out there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
